"""
utils/jersey_ocr.py

Utilities to extract and recognize jersey numbers from player bounding boxes using OCR.

Features:
- Supports EasyOCR (preferred) and pytesseract fallback.
- Multiple preprocessing attempts to improve OCR reliability.
- Returns best candidate text plus a simple confidence score.
- Batch helper to run over frames & player_tracks and return mapping frame->player_id->(number,conf).
"""

from typing import Tuple, Optional, Dict, List
import cv2
import numpy as np
import math
import os

# Try to import EasyOCR first (better for digits on complex backgrounds)
try:
    import easyocr
    _HAS_EASYOCR = True
except Exception:
    _HAS_EASYOCR = False

# pytesseract fallback
try:
    import pytesseract
    _HAS_PYTESSERACT = True
except Exception:
    _HAS_PYTESSERACT = False

# Small helper to ensure at least one OCR backend available
if not (_HAS_EASYOCR or _HAS_PYTESSERACT):
    # Not raising here so module can be imported; functions will raise when used.
    pass

def _ensure_reader(engine: str = "easyocr"):
    """
    Return an initialized reader for engine. engine in {"easyocr","pytesseract"}.
    For easyocr we cache the reader on module level to avoid repeated loads.
    """
    global _easyocr_reader
    if engine == "easyocr":
        if not _HAS_EASYOCR:
            raise RuntimeError("easyocr not installed; install via `pip install easyocr`")
        try:
            if " _easyocr_reader" not in globals() or _easyocr_reader is None:
                # English digits model; restrict to digits for speed/accuracy
                _easyocr_reader = easyocr.Reader(['en'], gpu=False, verbose=False)
            return _easyocr_reader
        except Exception as e:
            # propagate
            raise
    elif engine == "pytesseract":
        if not _HAS_PYTESSERACT:
            raise RuntimeError("pytesseract not installed; install via `pip install pytesseract` and system tesseract")
        return None
    else:
        raise ValueError("Unknown OCR engine: " + str(engine))


def _preprocess_for_ocr(img: np.ndarray, method: int = 0) -> np.ndarray:
    """
    Preprocess cropped jersey image for OCR. `method` selects different pipelines.
    Always returns a BGR or grayscale image suitable for OCR.
    Methods:
      0: basic gray + bilateral + adaptive threshold
      1: increase contrast (CLAHE) + threshold
      2: morphological opening + resize + sharpen
      3: color isolation (try to remove background by kmeans)
    """
    if img is None or img.size == 0:
        return img

    # Work on a copy
    im = img.copy()

    # Convert to gray
    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)

    if method == 0:
        # smooth and adaptive threshold
        gray = cv2.bilateralFilter(gray, 9, 75, 75)
        th = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                   cv2.THRESH_BINARY_INV, 15, 7)
        return th

    if method == 1:
        # CLAHE then Otsu
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        cl = clahe.apply(gray)
        blur = cv2.GaussianBlur(cl, (3,3), 0)
        _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        # invert such that digits are black on white for pytesseract (if needed)
        th = cv2.bitwise_not(th)
        return th

    if method == 2:
        # sharpen & morphological
        kernel = np.array([[0,-1,0],[-1,5,-1],[0,-1,0]])
        sharp = cv2.filter2D(gray, -1, kernel)
        resized = cv2.resize(sharp, (max(64, sharp.shape[1]*2), max(64, sharp.shape[0]*2)), interpolation=cv2.INTER_LINEAR)
        _, th = cv2.threshold(resized, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        return th

    if method == 3:
        # attempt to isolate bright numbers vs dark jersey via simple contrast stretch
        # normalize and equalize
        norm = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX)
        equ = cv2.equalizeHist(norm)
        _, th = cv2.threshold(equ, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        th = cv2.bitwise_not(th)
        return th

    # default: return grayscale
    return gray


def _deskew(img: np.ndarray) -> np.ndarray:
    """Try to deskew using minAreaRect if text is rotated slightly."""
    if img is None or img.size == 0:
        return img
    # Only works on binary image
    try:
        if len(img.shape) == 3:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        else:
            gray = img
        _, bw = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        coords = np.column_stack(np.where(bw > 0))
        if coords.shape[0] < 10:
            return img
        rect = cv2.minAreaRect(coords)
        angle = rect[-1]
        if angle < -45:
            angle = -(90 + angle)
        else:
            angle = -angle
        (h, w) = img.shape[:2]
        center = (w // 2, h // 2)
        M = cv2.getRotationMatrix2D(center, angle, 1.0)
        rotated = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
        return rotated
    except Exception:
        return img


def _ocr_with_easyocr(reader, img: np.ndarray, allow_digits_only=True) -> List[Tuple[str, float]]:
    """
    Run easyocr on img and return list of (text, confidence) candidates.
    We return one or more candidates as found by EasyOCR.
    """
    results = []
    if img is None or img.size == 0:
        return results
    # EasyOCR expects color images or grayscale fine
    try:
        out = reader.readtext(img, detail=1)  # returns list [(bbox, text, conf), ...]
        for bbox, text, conf in out:
            # postprocess text: keep digits and a bit of filtering
            txt = text.strip()
            if allow_digits_only:
                txt = "".join([c for c in txt if c.isdigit()])
                if txt == "":
                    continue
            results.append((txt, float(conf)))
    except Exception:
        pass
    return results


def _ocr_with_pytesseract(img: np.ndarray, psm: int = 7, oem: int = 3, allow_digits_only=True) -> List[Tuple[str, float]]:
    """
    Run pytesseract on image. Returns list of (text, pseudo_confidence).
    psm: page segmentation mode
    oem: OCR engine mode
    Note: pytesseract's confidences are trickier; we compute simple heuristic confidence.
    """
    results = []
    if img is None or img.size == 0:
        return results
    config = f'--oem {oem} --psm {psm} -c tessedit_char_whitelist=0123456789'
    try:
        data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT, config=config)
        n = len(data['text'])
        for i in range(n):
            txt = data['text'][i].strip()
            if txt == "":
                continue
            if allow_digits_only:
                txt = "".join([c for c in txt if c.isdigit()])
                if txt == "":
                    continue
            # tesseract gives confidences as int in 'conf'
            conf = float(data['conf'][i]) if str(data['conf'][i]).replace('-', '').isdigit() else 0.0
            # normalize negative or weird conf
            if conf < 0:
                conf = 0.0
            results.append((txt, conf / 100.0))
    except Exception:
        pass
    return results


def recognize_jersey_number_from_crop(crop: np.ndarray,
                                      ocr_engine: str = "easyocr",
                                      max_attempts: int = 4) -> Tuple[Optional[str], float]:
    """
    Recognize jersey number from a cropped image (player's jersey region).

    Returns:
        best_text (str or None), confidence (0..1)
    """
    if crop is None or crop.size == 0:
        return None, 0.0

    # ensure reader
    # We don't raise here if backend missing; handle gracefully
    reader = None
    if ocr_engine == "easyocr" and _HAS_EASYOCR:
        reader = _ensure_reader("easyocr")
    elif ocr_engine == "pytesseract" and _HAS_PYTESSERACT:
        reader = _ensure_reader("pytesseract")
    else:
        # try easyocr first, then pytesseract
        if _HAS_EASYOCR:
            reader = _ensure_reader("easyocr")
            ocr_engine = "easyocr"
        elif _HAS_PYTESSERACT:
            reader = _ensure_reader("pytesseract")
            ocr_engine = "pytesseract"
        else:
            raise RuntimeError("No OCR backend available. Install easyocr or pytesseract + tesseract.")

    # Attempt multiple preprocess methods and deskewing to maximize chance
    candidates: List[Tuple[str, float]] = []
    for method in range(max_attempts):
        proc = _preprocess_for_ocr(crop, method=method)
        proc = _deskew(proc)

        if ocr_engine == "easyocr":
            out = _ocr_with_easyocr(reader, proc, allow_digits_only=True)
        else:
            out = _ocr_with_pytesseract(proc, psm=7, oem=3, allow_digits_only=True)

        # Collect results
        for t, c in out:
            if t is None or t == "":
                continue
            # canonicalize: remove leading zeros unless standalone zero
            t_norm = t.lstrip("0") or "0"
            candidates.append((t_norm, float(c)))

    if not candidates:
        return None, 0.0

    # choose best candidate by confidence then by digit-length (prefer 1-2 digits, common on jerseys)
    candidates = sorted(candidates, key=lambda x: (x[1], len(str(x[0]))), reverse=True)
    best_text, best_conf = candidates[0]
    # constrain to reasonable jersey numbers (1..99)
    try:
        n = int(best_text)
        if not (0 <= n <= 99):
            return None, 0.0
    except Exception:
        return None, 0.0
    return str(int(best_text)), float(best_conf)


def crop_jersey_region(frame: np.ndarray, bbox: Tuple[int, int, int, int], region="upper") -> Optional[np.ndarray]:
    """
    Given a full player bbox (x1,y1,x2,y2), return a crop likely to contain the jersey number.
    Strategies:
      - Use upper half or center of bbox (numbers are usually on chest/back)
      - Optionally expand horizontally for visibility
    region: "upper" or "back" or "center"
    """
    if frame is None or bbox is None:
        return None
    try:
        x1, y1, x2, y2 = [int(float(v)) for v in bbox[:4]]
    except Exception:
        return None
    h, w = frame.shape[:2]
    # clamp
    x1 = max(0, min(x1, w-1))
    x2 = max(0, min(x2, w-1))
    y1 = max(0, min(y1, h-1))
    y2 = max(0, min(y2, h-1))
    if x2 <= x1 or y2 <= y1:
        return None

    bw = x2 - x1
    bh = y2 - y1

    if region == "upper":
        # upper 40%-65% of bbox (chest/upper back)
        top = y1 + int(0.12 * bh)
        bottom = y1 + int(0.55 * bh)
        left = x1 + int(0.05 * bw)
        right = x2 - int(0.05 * bw)
    elif region == "center":
        top = y1 + int(0.25 * bh)
        bottom = y1 + int(0.75 * bh)
        left = x1 + int(0.05 * bw)
        right = x2 - int(0.05 * bw)
    else:
        # "back" fallback same as upper
        top = y1 + int(0.12 * bh)
        bottom = y1 + int(0.55 * bh)
        left = x1 + int(0.05 * bw)
        right = x2 - int(0.05 * bw)

    # expand a little
    pad_x = int(0.02 * bw) + 1
    pad_y = int(0.02 * bh) + 1
    lx = max(0, left - pad_x)
    rx = min(w, right + pad_x)
    ty = max(0, top - pad_y)
    by = min(h, bottom + pad_y)

    if rx <= lx or by <= ty:
        return None

    crop = frame[ty:by, lx:rx].copy()
    # if crop is tiny, resize to a reasonable size
    if crop.shape[0] < 24 or crop.shape[1] < 24:
        try:
            crop = cv2.resize(crop, (max(48, crop.shape[1]*2), max(48, crop.shape[0]*2)), interpolation=cv2.INTER_LINEAR)
        except Exception:
            pass
    return crop


def recognize_jersey_number_on_frame(frame: np.ndarray,
                                     player_bbox: Tuple[int, int, int, int],
                                     ocr_engine: str = "easyocr") -> Tuple[Optional[str], float]:
    """
    Convenience wrapper: crop an estimated jersey region and run OCR.
    Returns recognized number string (or None) and confidence 0..1.
    """
    crop = crop_jersey_region(frame, player_bbox, region="upper")
    if crop is None:
        return None, 0.0
    return recognize_jersey_number_from_crop(crop, ocr_engine=ocr_engine, max_attempts=4)


def batch_recognize_over_video(frames: List[np.ndarray],
                               player_tracks: List[Dict[int, Dict]],
                               ocr_engine: str = "easyocr") -> List[Dict[int, Tuple[Optional[str], float]]]:
    """
    Run jersey OCR for all frames and players.
    Returns list of length = len(frames) where each element is dict player_id -> (number_str_or_None, conf)
    This function is defensive and will skip invalid frames/tracks.
    """
    n = len(frames)
    out = [dict() for _ in range(n)]
    for i in range(n):
        frame = frames[i]
        players = {}
        try:
            players = player_tracks[i] or {}
        except Exception:
            players = {}
        for pid, pdata in (players or {}).items():
            try:
                bbox = pdata.get("bbox")
                if not bbox:
                    out[i][pid] = (None, 0.0)
                    continue
                num, conf = recognize_jersey_number_on_frame(frame, bbox, ocr_engine=ocr_engine)
                out[i][pid] = (num, conf)
            except Exception:
                out[i][pid] = (None, 0.0)
    return out


def aggregate_player_numbers(frame_results: List[Dict[int, Tuple[Optional[str], float]]],
                             min_confidence: float = 0.45,
                             min_occurrence: int = 2) -> Dict[int, str]:
    """
    Aggregate per-frame OCR outputs into stable player_id -> jersey_number mapping.
    Policy:
      - For each player_id, collect recognized numbers with confidences.
      - Keep numbers with confidence >= min_confidence.
      - Choose number with most occurrences across frames; require min_occurrence.
    Returns mapping player_id -> number_str (only if satisfied), else not present.
    """
    from collections import defaultdict, Counter
    occurrences = defaultdict(list)
    for fr in frame_results:
        for pid, (num, conf) in fr.items():
            if num is None:
                continue
            if conf >= min_confidence:
                occurrences[int(pid)].append(num)

    final = {}
    for pid, nums in occurrences.items():
        if not nums:
            continue
        cnt = Counter(nums)
        number, count = cnt.most_common(1)[0]
        if count >= min_occurrence:
            final[pid] = number
    return final




    """
TeamAssigner
Assign players to teams using either CLIP (preferred) or a fast k-means color heuristic (fallback).

Public API (keeps compatibility with your pipeline):
    get_player_teams_across_frames(video_frames, player_tracks, read_from_stub=False, stub_path=None)

Notes:
- If transformers + CLIP are installed and available, CLIP will be used unless `fast_mode=True`.
- If CLIP is unavailable or fails to load, the code falls back to a k-means color heuristic.
- The function caches results (using your existing read_stub/save_stub functions) if a `stub_path` is provided.
"""

import os
import sys
from typing import Tuple, Dict, List, Optional, Any
from collections import defaultdict, Counter

import numpy as np
import cv2
from PIL import Image

# Try to import CLIP + torch lazily. If missing, we fallback to fast kmeans method.
try:
    from transformers import CLIPProcessor, CLIPModel
    import torch
    _HAS_CLIP = True
except Exception:
    _HAS_CLIP = False

# Ensure repo utils are importable (keeps same repo layout).
folder = os.path.dirname(__file__)
sys.path.append(os.path.join(folder, "../"))  # noqa: E402

# Assuming your repository structure makes 'utils' available for import
try:
    from utils import read_stub, save_stub  # your existing utils (pickle stubs)
except ImportError:
    print("[ERROR] Could not import read_stub/save_stub from utils. Check your path setup.")
    # Define dummy functions to prevent crash if running locally without full repo setup
    def read_stub(*args, **kwargs): return None
    def save_stub(*args, **kwargs): pass


def _kmeans_color_label(image_crop: Optional[np.ndarray], k: int = 2) -> str:
    """
    Fallback labeling using k-means color clustering on the player crop.
    Returns either "white shirt" or "dark blue shirt" (to match default labels),
    or "unknown" if the crop is invalid.
    """
    if image_crop is None or image_crop.size == 0:
        return "unknown"
    try:
        # Ensure we have a 3-channel BGR image
        if len(image_crop.shape) == 2:
            image_crop = cv2.cvtColor(image_crop, cv2.COLOR_GRAY2BGR)
        h, w = image_crop.shape[:2]
        data = image_crop.reshape((-1, 3)).astype(np.float32)

        # If image too small, resize to reduce noise
        if h * w < 100:
            image_crop = cv2.resize(image_crop, (64, 128), interpolation=cv2.INTER_LINEAR)
            data = image_crop.reshape((-1, 3)).astype(np.float32)

        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
        flags = cv2.KMEANS_RANDOM_CENTERS
        _, labels, centers = cv2.kmeans(data, k, None, criteria, 10, flags)
        counts = np.bincount(labels.flatten(), minlength=k)
        dominant = centers[np.argmax(counts)]  # BGR center

        # perceived brightness from RGB
        b, g, r = float(dominant[0]), float(dominant[1]), float(dominant[2])
        brightness = 0.299 * r + 0.587 * g + 0.114 * b

        return "white shirt" if brightness > 100 else "dark blue shirt"
    except Exception:
        return "unknown"


class TeamAssigner:
    """
    Assign players to teams using CLIP (if available) or a k-means color heuristic.

    Args:
        team_1_class_name: textual label representing team 1 (default "white shirt")
        team_2_class_name: textual label representing team 2 (default "dark blue shirt")
        fast_mode: if True, skip CLIP entirely and always use k-means heuristic (good offline/fast)
        device: "cpu" or "cuda" if using CLIP; if None auto-detects
    """

    def __init__(
        self,
        team_1_class_name: str = "white shirt",
        team_2_class_name: str = "dark blue shirt",
        fast_mode: bool = False,
        device: Optional[str] = None,
    ):
        self.team_1_class_name = team_1_class_name
        self.team_2_class_name = team_2_class_name
        self.player_team_dict: Dict[int, int] = {}
        self.team_colors: Dict[int, Tuple[int, int, int]] = {}
        self.fast_mode = bool(fast_mode)

        # device selection for CLIP
        if device is not None:
            self.device = device
        else:
            if _HAS_CLIP:
                try:
                    self.device = "cuda" if torch.cuda.is_available() else "cpu"
                except Exception:
                    self.device = "cpu"
            else:
                self.device = "cpu"

        # CLIP objects (loaded lazily)
        self.model: Optional[Any] = None
        self.processor: Optional[Any] = None
        self._model_loaded = False

    def load_model(self) -> bool:
        """
        Attempt to load CLIP model + processor.
        Returns True if loaded successfully.
        """
        if self.fast_mode:
            print("[DBG] TeamAssigner: fast_mode enabled — skipping CLIP model load")
            return False

        if not _HAS_CLIP:
            print("[DBG] TeamAssigner: transformers/CLIP not available — will use fallback.")
            return False

        if self._model_loaded:
            return True

        try:
            print("[DBG] TeamAssigner: loading CLIP model (may take time)...")
            # Use the same model as original code if available
            self.model = CLIPModel.from_pretrained("patrickjohncyh/fashion-clip")
            self.processor = CLIPProcessor.from_pretrained("patrickjohncyh/fashion-clip")
            # Move model to device if possible
            try:
                self.model.to(self.device)
            except Exception:
                pass
            self._model_loaded = True
            print(f"[DBG] TeamAssigner: CLIP loaded on device={self.device}")
            return True
        except Exception as e:
            print(f"[DBG] TeamAssigner: failed to load CLIP model: {e} — falling back to kmeans")
            self._model_loaded = False
            return False

    def _crop_safe(self, frame: Optional[np.ndarray], bbox: Optional[List[float]], player_id: int = -1, frame_num: int = -1) -> Optional[np.ndarray]:
        """
        Safely crop bbox from frame. Returns None if invalid.
        Ensures bounding box clipping inside the frame and minimal crop size, with debug logging.
        """
        if frame is None or bbox is None:
            if player_id != -1 and frame_num != -1:
                # Log when frame or bbox is explicitly None
                print(f"[DBG] TeamAssigner: crop invalid (frame/bbox is None) for player {player_id} frame {frame_num}")
            return None
        try:
            x1, y1, x2, y2 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])
            h, w = frame.shape[:2]
            x1 = max(0, min(x1, w - 1))
            x2 = max(0, min(x2, w - 1))
            y1 = max(0, min(y1, h - 1))
            y2 = max(0, min(y2, h - 1))
            if x2 <= x1 or y2 <= y1:
                if player_id != -1 and frame_num != -1:
                    print(f"[DBG] TeamAssigner: crop invalid (bbox zero-size) for player {player_id} frame {frame_num}")
                return None
            crop = frame[y1:y2, x1:x2]
            # If crop very small, resize to a reasonable size for processing
            ch, cw = crop.shape[:2]
            
            # Injecting the requested debug logging based on crop size
            if ch < 20 or cw < 20:
                if player_id != -1 and frame_num != -1:
                    print(f"[DBG] TeamAssigner: crop too small ({crop.shape[0]}x{crop.shape[1]}) for player {player_id} frame {frame_num}")
                crop = cv2.resize(crop, (64, 128), interpolation=cv2.INTER_LINEAR)
            
            return crop
        except Exception as e:
            if player_id != -1 and frame_num != -1:
                print(f"[DBG] TeamAssigner: _crop_safe exception for player {player_id} frame {frame_num}: {e}")
            return None

    def get_player_color_clip(self, frame: np.ndarray, bbox: List[float], player_id: int = -1, frame_num: int = -1) -> str:
        """
        Use CLIP to classify the player's crop between the two class names.
        Returns one of the class labels or "unknown".
        """
        if not self._model_loaded:
            if not self.load_model():
                return "unknown"

        # Pass context for logging
        crop = self._crop_safe(frame, bbox, player_id, frame_num)
        if crop is None:
            return "unknown"

        try:
            # convert BGR -> RGB PIL
            rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)
            pil = Image.fromarray(rgb)
            classes = [self.team_1_class_name, self.team_2_class_name]
            inputs = self.processor(text=classes, images=pil, return_tensors="pt", padding=True)
            # Move tensors to device selected for model if possible
            if hasattr(inputs, "to"):
                try:
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                except Exception:
                    pass
            outputs = self.model(**inputs)
            logits = outputs.logits_per_image  # shape (1, n_classes)
            probs = logits.softmax(dim=1)
            idx = int(probs.argmax(dim=1)[0])
            return classes[idx]
        except Exception as e:
            print(f"[DBG] TeamAssigner: CLIP classification error: {e}")
            return "unknown"

    def get_player_color(self, frame: np.ndarray, bbox: List[float], player_id: int = -1, frame_num: int = -1) -> str:
        """
        Public method that returns a textual class for the player's crop.
        Uses CLIP when available and not in fast_mode; otherwise k-means fallback.
        """
        crop = self._crop_safe(frame, bbox, player_id, frame_num)
        
        if self.fast_mode:
            # Pass crop directly to k-means to avoid re-cropping
            return _kmeans_color_label(crop)

        # try CLIP first (if available) - passing context
        label = self.get_player_color_clip(frame, bbox, player_id, frame_num)
        if label is None or label == "unknown":
            # fallback to kmeans - passing already cropped image
            label = _kmeans_color_label(crop)
        return label

    def get_player_team(self, frame: np.ndarray, player_bbox: List[float], player_id: int) -> int:
        """
        Return team id (1 or 2) for a single player, using cached mapping if possible.
        Defaults to team 2 if classification uncertain.
        
        NOTE: This method is primarily for compatibility; the majority-vote logic is preferred.
        """
        if player_id in self.player_team_dict:
            return self.player_team_dict[player_id]

        # Note: frame_num is not available here, so we skip passing it to get_player_color
        label = self.get_player_color(frame, player_bbox, player_id=player_id)
        team_id = 2
        if label == self.team_1_class_name:
            team_id = 1
        elif label == "unknown":
            # fallback to brightness heuristic
            crop = self._crop_safe(frame, player_bbox, player_id=player_id)
            if crop is not None:
                b, g, r = cv2.mean(crop)[:3]
                brightness = 0.299 * r + 0.587 * g + 0.114 * b
                if brightness > 100:
                    team_id = 1
        # cache this mapping for the player id (resets periodically in main pipeline)
        self.player_team_dict[player_id] = team_id
        return team_id

    def get_player_teams_across_frames(
        self,
        video_frames: List[np.ndarray],
        player_tracks: List[Dict[int, Dict]],
        read_from_stub: bool = False,
        stub_path: Optional[str] = None,
        min_votes: int = 3
    ) -> List[Dict[int, int]]:
        """
        New approach:
        - For every player occurrence across frames, collect label votes ("team1","team2","unknown")
        - After processing frames, compute a stable team id for each player by majority vote
        - Return per-frame mapping where every player's team is the stable team id (so visualization is stable)
        """
        
        # Try to load per-player stable mapping from stub if available
        stable_map = {}
        if stub_path:
            try:
                # The stub now saves the stable_map dict, not the per-frame list
                st = read_stub(read_from_stub, stub_path)
                if st is not None and isinstance(st, dict):
                    stable_map = st
                    print(f"[DBG] TeamAssigner: loaded stable_map from stub with {len(stable_map)} players")
            except Exception:
                pass

        n_frames = len(video_frames)
        
        # If stable_map exists, apply to all frames
        if stable_map and len(stable_map) > 0 and player_tracks:
            out = []
            for frame_tracks in player_tracks:
                frame_map = {}
                for pid_raw in (frame_tracks or {}).keys():
                    try:
                        pid = int(pid_raw)
                    except Exception:
                        pid = pid_raw
                    # Use stable map, default to Team 2 if player ID is new or missing
                    frame_map[pid] = stable_map.get(pid, 2) 
                out.append(frame_map)
            # Ensure output length matches video frames
            if len(out) == n_frames:
                 return out
            else:
                 print(f"[WARN] TeamAssigner: Loaded stable map but player_tracks length mismatch ({len(player_tracks)} vs {n_frames}). Recalculating.")


        # Ensure model loaded if available (CLIP)
        if not self.fast_mode and _HAS_CLIP:
            self.load_model()
            
        # gather votes
        votes = defaultdict(list)  # pid -> list of team ids (1 or 2 or 0 unknown)
        
        for fi, frame_tracks in enumerate(player_tracks or []):
            if not frame_tracks:
                continue
            # Safety check for frame index
            if fi >= n_frames:
                 print(f"[WARN] Frame index {fi} out of bounds for video frames. Stopping vote collection.")
                 break
            frame = video_frames[fi]
            
            for pid_raw, pdata in (frame_tracks or {}).items():
                try:
                    pid = int(pid_raw)
                except Exception:
                    # Use raw ID if conversion fails
                    pid = pid_raw 
                bbox = pdata.get("bbox") if isinstance(pdata, dict) else None
                if bbox is None:
                    continue
                
                # Get label (CLIP or K-means) - passing frame context for debugging
                label = self.get_player_color(frame, bbox, pid, fi)
                
                # map label to team id
                if label == self.team_1_class_name:
                    tid = 1
                elif label == self.team_2_class_name:
                    tid = 2
                else:
                    # unknown -> fallback heuristic brightness (same as get_player_team)
                    crop = self._crop_safe(frame, bbox)
                    if crop is not None and crop.size > 0:
                        b,g,r = cv2.mean(crop)[:3]
                        brightness = 0.299*r + 0.587*g + 0.114*b
                        tid = 1 if brightness > 100 else 2
                    else:
                        tid = 0 # 0 for truly uncertain/missing data
                
                votes[pid].append(tid)

        # build stable_map by majority vote, require min_votes appearances
        stable_map = {}
        for pid, arr in votes.items():
            total_votes = len(arr)
            if total_votes < min_votes:
                # not enough evidence; skip (will default to team 2 later)
                print(f"[DBG] TeamAssigner: Player {pid} skipped - only {total_votes} votes (min={min_votes})")
                continue
            
            cnt = Counter(arr)
            
            # Simple majority logic:
            count_team_1 = cnt.get(1, 0)
            count_team_2 = cnt.get(2, 0)
            
            if count_team_1 > count_team_2:
                team_choice = 1
            elif count_team_2 > count_team_1:
                team_choice = 2
            else:
                # Tie or all votes were 0 (unknown) - default to 2 
                # Note: We rely on the final per-frame mapping to apply the default '2'
                continue 

            stable_map[int(pid)] = int(team_choice)
        
        print(f"[INFO] TeamAssigner: Stable map generated for {len(stable_map)} players.")

        # build per-frame mapping using stable map
        final_per_frame = []
        for fi in range(n_frames):
            frame_map = {}
            # Ensure player_tracks has a valid list/dict for the frame index
            if fi < len(player_tracks) and player_tracks[fi]:
                for pid_raw in (player_tracks[fi] or {}).keys():
                    try:
                        pid = int(pid_raw)
                    except Exception:
                        pid = pid_raw
                    # Use stable map, default to Team 2 if player ID is new or missing
                    frame_map[pid] = stable_map.get(pid, 2)
            final_per_frame.append(frame_map)

        # save stable map to stub for future runs
        if stub_path:
            try:
                # Save the stable_map dict, not the per-frame list
                save_stub(stub_path, stable_map)
                print(f"[DBG] TeamAssigner: saved stable_map stub to {stub_path}")
            except Exception as e:
                print(f"[DBG] TeamAssigner: failed to save stub {stub_path}: {e}")

        return final_per_frame